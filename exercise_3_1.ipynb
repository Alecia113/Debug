{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise_3.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqybvq+JSHIwx+Msk4naXs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/Debug/blob/main/exercise_3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJPx2i5C0Bfc"
      },
      "source": [
        "# Exercise\n",
        "Please complete the following **two questions** E1 and E2 and and submit your **\"ipynb\" file to Canvas**. (You can download it using \"File\" > \"Download .ipynb\"). Please make sure all code required to run your solution properly must be included in the submission file.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF-vRmAYVpMk"
      },
      "source": [
        "##E1. Briefly describe the impact of window size selection on the Word2Vec?\n",
        "Please write down your answer below with your own words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jMj_HobwXgR8"
      },
      "source": [
        "#@Lab01 - E1\n",
        "\n",
        "Answer = \" 小的window size在单词可以互换的情况下，我们可以提高word2Vec模型的相似度。\" #@param {type:\"raw\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq26RDjdzTpJ"
      },
      "source": [
        "##E2. CBOW implementation with PyTorch nn.Module and torch.optim\n",
        "\n",
        "In the tutorial, we learned how to train a word2vec skip-gram model in pytorch with either manually updates the parameters (weights) in it or using nn.Module and torch.optim. \n",
        "\n",
        "In this Lab 03 E2, please:\n",
        "\n",
        "1.   use the \"NN Model (**nn.Module**)\" and the \"Optimiser (**torch.optim**)\" (that we learned in the above sections) to train a word2vec **CBOW (NOT Skip Gram)** model  on the provided toy data with widow_size=1 and embedding_size=2. And you are also encouraged to research on **nn.Embedding** and use it in this exercise but it is not required.\n",
        "2.   visualize (plot) the trained embeddings for each word in the vocabulary\n",
        "\n",
        "\n",
        "\n",
        "Note: The embedding size should 2. The code for the preprocessing and the hyperparameter setup are provided. Have fun!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFMCbPv62EXA"
      },
      "source": [
        "'''\n",
        "PyTorch nn.module 和toch.optim 实现CBOW [多到一]\n",
        "\n",
        "我们会nn.Module 和torch.optim 来训练word2Vec skip-gram\n",
        "=======\n",
        "\n",
        "1：使用 nn.Module 和优化器 torch.optim 去训练 word2vec CBOW [这个地方变了]\n",
        "window_size = 1 ; embedding_size = 2 (可研究nn.Embedding，不必要)\n",
        "\n",
        "2plot 每个词。\n",
        "\n",
        "潜入大小2； 提供了预处理和超参数设置的代码\n",
        "\n",
        "'''\n",
        "'''\n",
        "nn.model 就是一个个转头堆成神经网络\n",
        "而word2vec 就需要两个线性回归就可做出来了\n",
        "\n",
        "linear （参数 神经元：work——size——hidden.size 到vecbolary.size)\n",
        "window就是为了做中center和context的呀\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if-vXfWy2LKk"
      },
      "source": [
        "#先回去把它给你的语句弄明白是什么意思\n",
        "#看看缺的是什么\n",
        "#明确这两个东西 nn.module 和torch.optim\n",
        "#明确流程是哪一步\n",
        "#不行先从没有隐藏层开始\n",
        "#可以先听课\n",
        "#回去就开始研究再研究会"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkqaE_i_WQL2"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F  #add\n",
        "from random import shuffle\n",
        "\n",
        "# Raw data - sentences\n",
        "# Let's create toy data for simplicity \n",
        "sentences = [\"he likes cat\",\n",
        "             \"he likes dog\",\n",
        "             \"he likes animal\",\n",
        "             \"dog cat animal\",\n",
        "             \"she likes cat\",\n",
        "             \"she dislikes dog\",\n",
        "             \"cat likes fish\",\n",
        "             \"cat likes milk\",\n",
        "             \"dog likes bone\",\n",
        "             \"dog dislikes fish\",\n",
        "             \"dog likes milk\",\n",
        "             \"she likes movie\",\n",
        "             \"she likes music\",\n",
        "             \"he likes game\",\n",
        "             \"he likes movie\",\n",
        "             \"cat dislikes dog\"]\n",
        "\n",
        "# convert all sentences to unique word list\n",
        "word_list = \" \".join(sentences).split()     #将所有句子转换为唯一单词列表  #每个单词拆出来\n",
        "word_list = list(set(word_list))    #把重复的单词去掉 ，每个单词都取一个\n",
        "\n",
        "\n",
        "# make dictionary so that we can reference each index of unique word\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}      # 制作字典，以便我们可以在dex中引用每个单词；格式为w(词)：i(序号) \n",
        "\n",
        "# make window size=1 for cbow\n",
        "# i.e.) he likes cat\n",
        "#   -> ([likes], he), ([he, cat], likes), ([likes], cat)\n",
        "#   -> ([likes, likes], he), ([he, cat], likes), ([likes, likes], cat)\n",
        "# Double the input when the word doesn't have two neighbours\n",
        "# This will make your input have the same size, which will make it easier when you write the CBOW model code\n",
        "# But this trick only works when window_size = 1\n",
        "\n",
        "cbow = [] #不同#初始化cbow\n",
        "#开始循环 在做window 跳步 \n",
        "#有点迭代的感觉。\n",
        "for sentence in sentences:\n",
        "    sentence = sentence.split()\n",
        "    for i in range(len(sentence)):\n",
        "        centre = word_dict[sentence[i]]\n",
        "        if i > 0 and i < len(sentence)-1:\n",
        "            context = [word_dict[sentence[i - 1]], word_dict[sentence[i + 1]]]  #中间部分\n",
        "        elif i == 0:\n",
        "            context = [word_dict[sentence[i + 1]], word_dict[sentence[i + 1]]]  #右边1\n",
        "        else:\n",
        "            context = [word_dict[sentence[i - 1]], word_dict[sentence[i - 1]]]  #左边一个\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        cbow.append([context, centre]) #不同\n",
        "\n",
        "# hyperparameter\n",
        "voc_size = len(word_list)#13\n",
        "learning_rate = 0.1\n",
        "batch_size = 16   #crossentropyloss(16*13)\n",
        "embedding_size = 2\n",
        "no_of_epochs = 5000\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddcQeRVdSnui"
      },
      "source": [
        "cbow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jumdA525J7Lh",
        "outputId": "40b83464-fc1e-4644-be5b-e59748c8c9f6"
      },
      "source": [
        "word_dict"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'animal': 2,\n",
              " 'bone': 6,\n",
              " 'cat': 3,\n",
              " 'dislikes': 9,\n",
              " 'dog': 5,\n",
              " 'fish': 11,\n",
              " 'game': 7,\n",
              " 'he': 10,\n",
              " 'likes': 0,\n",
              " 'milk': 8,\n",
              " 'movie': 1,\n",
              " 'music': 4,\n",
              " 'she': 12}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16dGiQwCLQgk",
        "outputId": "05b40c17-c4db-4b46-da5d-971df0c52f60"
      },
      "source": [
        "context"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 8]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Eoia1NLSmI",
        "outputId": "3c4ef14d-0ece-4538-fa16-eca838663a5b"
      },
      "source": [
        "centre"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0g3YwTsJact"
      },
      "source": [
        "#text\n",
        "cbow  #skip_grams 是两位 这里是三位多一维\n",
        "'''\n",
        "[[[8, 8], 1],\n",
        " [[1, 4], 8],\n",
        " '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-ZkBqDT6I30"
      },
      "source": [
        "voc_size = len(word_list)       # 不重复的单词一共13; 13\n",
        "\n",
        "def prepare_batch(data_temp):     #[[[8, 8], 4], [[9, 5], 7], [[1, 12], 8], [[8, 8], 3], [[8, 8], 11], ...] (16 items total)\n",
        "  inputs = []\n",
        "  labels = []\n",
        "\n",
        "\n",
        "  for i in range(len(data_temp)):\n",
        "    input_temp = [0]*voc_size     #让这13个不同的单词位置上都是0;#[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]s[固定]\n",
        "    input_temp[data_temp[i][0][0]] = 1   # one-hot input 【？？】是指【input】【labels】么？#不确定\n",
        "    inputs.append(input_temp)   # centre\n",
        "    labels.append(data_temp[i][1])  #context word\n",
        "\n",
        "  return np.array(inputs), np.array(labels)\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx1EXOD6ppPR",
        "outputId": "18ec0b44-7524-409f-d512-9fe344f6adca"
      },
      "source": [
        "data_temp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[9, 9], 9],\n",
              " [[9, 9], 9],\n",
              " [[11, 0], 0],\n",
              " [[4, 3], 4],\n",
              " [[4, 0], 4],\n",
              " [[9, 9], 9],\n",
              " [[9, 9], 9],\n",
              " [[9, 9], 9],\n",
              " [[5, 3], 3],\n",
              " [[5, 1], 5],\n",
              " [[4, 10], 10],\n",
              " [[8, 8], 8],\n",
              " [[4, 5], 5],\n",
              " [[9, 9], 9],\n",
              " [[9, 9], 9],\n",
              " [[9, 9], 9]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwQA6LGApcJW",
        "outputId": "fd5a320e-ecd6-48fa-a0d7-c4f576930027"
      },
      "source": [
        "labels = []\n",
        "labels.append(data_temp[i][1])\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIO1BIJ1pNXx",
        "outputId": "32e333e9-02ae-4a80-b25f-fc0b9aff5327"
      },
      "source": [
        "  input_temp = [0]*voc_size     #让这13个不同的单词位置上都是0;#[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]s[固定]\n",
        "  input_temp[data_temp[i][0][0]] = 1 \n",
        "  inputs_temp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A478bNqxq_5C",
        "outputId": "0a25ddf7-f3b5-4d98-dac6-ec8e4d591d79"
      },
      "source": [
        "#perpare for data\n",
        "x_data = np.array(\n",
        "    [[0,0],[1,0],[1,1],[0,0],[0,0],[0,1]])  #创建数据one-hot\n",
        "\n",
        "x_data_torch = torch.from_numpy(x_data).float() #转类型变张量，目前还是整数\n",
        "\n",
        "y_data = np.array([0,1,2,0,0,2]) #output\n",
        "y_data_torch = torch.from_numpy(y_data) #output torch\n",
        "\n",
        "num_features = 2 # 2个特征--hair, feather\n",
        "num_classes = 3 # 3 输出类 other,mammal,bird"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhaEsRJh1-pV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "5bb0cd95-0f00-432a-da64-0f351b7b05d6"
      },
      "source": [
        "for ind in range(0,len(cbow),batch_size):   #ind = epoch \n",
        "    data_temp = cbow[ind: min(ind+batch_size, len(cbow))]\n",
        "    inputs_temp, labels_temp = prepare_batch(data_temp)\n",
        "\n",
        "    inputs_torch = torch.from_numpy(inputs_temp).float()    #inputs\n",
        "    labels_torch = torch.from_numpy(labels_temp)  #把数组转成张量   #labels\n",
        "\n",
        "    hidden = torch.add(torch.matmul(inputs_torch,W1),B1)#还多了个hidden = z1\n",
        "    out = torch.add(torch.matmul(F.relu(hidden),Wout), Bout) #不一样是放进了激活函数\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2778040761be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlabels_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_temp\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#把数组转成张量   #labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_torch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#还多了个hidden = z1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#不一样是放进了激活函数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'W1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "QPCE8hQF1dMW",
        "outputId": "b56371bb-089d-41ff-9c3f-bc3c7fc35288"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "W1 = torch.randn(voc_size, embedding_size, requires_grad=True) #输入特征，输出（隐藏侧神经元数量）\n",
        "B1 = torch.randn(embedding_size, requires_grad=True) # 让隐藏层；偏移输出 候选日期的数量。即5 隐藏层中的神经元数量\n",
        "# w1 （ ……num_features， num_classes， requ）\n",
        "#Bout = 没隐藏的b\n",
        "Wout = torch.rand(embedding_size, num_classes, requires_grad=True)#输入神经元数，输出类\n",
        "\n",
        "Bout = torch.randn(num_classes, requires_grad=True)  #输出类3\n",
        "\n",
        "learning_rate=0.01 #没变\n",
        "no_of_epochs = 2000 # nob\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "  z1 = torch.add(torch.matmul(inputs_torch, W1), B1) # no 变\n",
        "  Zout = torch.add(torch.matmul(F.relu(z1), Wout), Bout) #  激活函数的加和\n",
        "\n",
        "  log_softmax = F.log_softmax(Zout,dim=1) #no大变\n",
        "  loss = F.nll_loss(log_softmax, labels_torch) #no大变\n",
        "\n",
        "  loss.backward()#nb\n",
        "  with torch.no_grad():#nb\n",
        "    W1.data -= learning_rate*W1.grad.data#nb\n",
        "    B1.data -= learning_rate*B1.grad.data#nb\n",
        "    \n",
        "    Wout.data -= learning_rate*Wout.grad.data  #bian\n",
        "    Bout.data -= learning_rate*Bout.grad.data  #b\n",
        "\n",
        "  W1.grad.data.zero_()#nb\n",
        "  B1.grad.data.zero_()#nb\n",
        "  Wout.grad.data.zero_()#b\n",
        "  Bout.grad.data.zero_()#b\n",
        "\n",
        "\n",
        "  loss_sum += loss.item()\n",
        "\n",
        "  if epoch % 500 == 499:\n",
        "    print('Epoch: %d, loss: %.4f' %(epoch +1 , loss_sum))\n",
        "\n",
        "\n",
        "print(\"Finished\") #分隔\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-5373b9d580ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mlog_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#no大变\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_torch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#no大变\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#nb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2383\u001b[0m         )\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2385\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2386\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 9 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG3uX7tu6Jdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "cdc53581-7659-4201-e77e-01e1a554c981"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "from random import shuffle\n",
        "import torch.optim as optim\n",
        "\n",
        "num_classes = 1 #j\n",
        "\n",
        "W1 = torch.randn(voc_size, embedding_size, requires_grad = True)  #还是随机生成\n",
        "Wout = torch.randn(embedding_size, num_classes, requires_grad=True)  #还是随机生成\n",
        "\n",
        "#j\n",
        "B1 = torch.randn(embedding_size, requires_grad= True)\n",
        "Bout = torch.randn(num_classes, requires_grad=True)\n",
        "\n",
        "class ModelWithHiddenLayer(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(ModelWithHiddenLayer, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    hidden = self.linear1(x)\n",
        "    out = self.linear2(F.relu(hidden))\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "model = ModelWithHiddenLayer(voc_size, embedding_size,num_classes)\n",
        "\n",
        "\n",
        "optimiser = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "  shuffle(cbow) #非必须\n",
        "  loss_sum = 0  #非必须\n",
        "#没懂在做什么\n",
        "  for ind in range(0,len(cbow),batch_size):   #ind = epoch \n",
        "    data_temp = cbow[ind: min(ind+batch_size, len(cbow))]\n",
        "    inputs_temp, labels_temp = prepare_batch(data_temp)\n",
        "\n",
        "    inputs_torch = torch.from_numpy(inputs_temp).float()    #inputs\n",
        "    labels_torch = torch.from_numpy(labels_temp)  #把数组转成张量   #labels\n",
        "\n",
        "    hidden = torch.add(torch.matmul(inputs_torch,W1),B1)#还多了个hidden = z1\n",
        "    out = torch.add(torch.matmul(F.relu(hidden),Wout), Bout) #不一样是放进了激活函数\n",
        "    '''\n",
        "    for epoch in range(no_of_epochs):\n",
        "  z1 = torch.add(torch.matmul(x_data_torch, W1), B1) # no 变\n",
        "  Zout = torch.add(torch.matmul(F.relu(z1), Wout), Bout) #  激活函数的加和\n",
        "\n",
        "  '''\n",
        "\n",
        "  '''\n",
        "  加了优化器就把这改了\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "    log_softmax = F.log_softmax(out,dim=1) #(out,dim=1)落东西了 #out = Zout\n",
        "    loss = F.nll_loss(log_softmax, labels_torch)    # labels_torch = y_data_torch\n",
        "  '''\n",
        "\n",
        "  model.train()\n",
        "  optimiser.zero_grad()\n",
        "\n",
        "  outputs = model(inputs_torch)\n",
        "  log_softmax = F.log_softmax(outputs,dim=1) #(out,dim=1)落东西了 #out = Zout\n",
        "  loss = F.nll_loss(log_softmax,labels_torch)    # labels_torch = y_data_torch\n",
        "\n",
        "\n",
        "   #参数改了一下 output , y_target\n",
        "  loss.backward()\n",
        "  #j\n",
        "  optimiser.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    W1.data -= learning_rate*W1.grad.data\n",
        "    Wout.data -= learning_rate*Wout.grad.data\n",
        "    #j\n",
        "    B1.data -= learning_rate*B1.grad.data\n",
        "    Bout.data -= learning_rate*Bout.grad.data\n",
        "\n",
        "  W1.grad.data.zero_()\n",
        "  Wout.grad.data.zero_()\n",
        "  #j\n",
        "  B1.grad.data.zero_()\n",
        "  Bout.grad.data.zero_()\n",
        "\n",
        "\n",
        "  loss_sum += loss.item()\n",
        "\n",
        "  if epoch % 500 == 499:\n",
        "    print('Epoch: %d, loss: %.4f' %(epoch +1 , loss_sum))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-d911db4f2a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0mlog_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(out,dim=1)落东西了 #out = Zout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_torch\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# labels_torch = y_data_torch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2383\u001b[0m         )\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2385\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2386\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 9 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8AF7Ijg7M3y",
        "outputId": "7dd5b3a0-c866-4dce-b484-07f13c518898"
      },
      "source": [
        "log_softmax.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYgxtadZ7RDG",
        "outputId": "44d45a90-7268-4b32-9549-a63f138f479f"
      },
      "source": [
        "labels_torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 9,  4,  4,  5,  6,  9,  5, 11,  9,  9, 10,  7,  9,  4,  5, 11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "8pDFdKFc6NpL",
        "outputId": "51d368af-bd56-4cd9-c56a-4654beb428d0"
      },
      "source": [
        "trained_embeddings = W1.data.numpy()\n",
        "\n",
        "for i, label in enumerate(word_list):  #enumerate 计算，列举\n",
        "  x, y = trained_embeddings[i]   \n",
        "  #print(label,\" : \", x, \" \", y)  #非必须\n",
        "  plt.scatter(x, y)\n",
        "  plt.annotate(label, xy=(x, y), xytext=(5,2), ##只有前两个名字都凑到一起了，就在点附近 xytext:所有的名字都集中在这个点\n",
        "               textcoords='offset points',ha= 'right',va='bottom')  #textcoords:名字又分到各自的点上了，有些点的名字改善了，能看清了\n",
        "          #ha 名字在点的左上方 va 没看出来有什么变化\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_JsECfq6Wji"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK_XU91R6Wvz"
      },
      "source": [
        "其他参考，期中得复习了"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x__yy4V1mNZo"
      },
      "source": [
        "### Your Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUPtvPgjmNGf"
      },
      "source": [
        "# Read the requirement carefully and implement the code by your own to fulfill the requirement.\n",
        "# You can refer to the code from the labs\n",
        "# Please try to make your code tidy with decent comments\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-k1W8RQIxO5"
      },
      "source": [
        "### Sample Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3TcOy0vAmsb",
        "outputId": "53a8be64-9e2e-4e89-9a2e-c590b6564d1a"
      },
      "source": [
        "# The sample output for model training log (only for format reference)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 500, loss: 4.4944\n",
            "Epoch: 1000, loss: 4.1465\n",
            "Epoch: 1500, loss: 4.0476\n",
            "Epoch: 2000, loss: 4.0518\n",
            "Epoch: 2500, loss: 4.0087\n",
            "Epoch: 3000, loss: 4.0032\n",
            "Epoch: 3500, loss: 4.0064\n",
            "Epoch: 4000, loss: 4.0014\n",
            "Epoch: 4500, loss: 4.0442\n",
            "Epoch: 5000, loss: 4.0085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkWJO3LkI-I_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "918a2013-f171-416e-f3ca-ea00be9e3fc7"
      },
      "source": [
        "# The sample output for trained embedding visualization (only for format reference)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSr_TGwd4bcE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPJUDZIe4bpU"
      },
      "source": [
        "#有关gensim.models.word2vec的更多详细信息，您可以参考Gensim Word2Vec的API。\n",
        "#https://radimrehurek.com/gensim/models/word2vec.html\n",
        "\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5 , workers=2, sg=0)\n",
        "\n",
        "'''\n",
        "Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, \n",
        "min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, \n",
        "min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, \n",
        "hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, \n",
        "sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
        "'''\n",
        "'''\n",
        " Initialize and train a word2vec model with the following parameters:\n",
        " 初始化+训练w2v模型\n",
        "# sentence: iterable of iterables, i.e. the list of lists of tokens from our data\n",
        "可迭代的迭代： 符号化来自我们数据的列表们的列表\n",
        "# size: dimensionality of the word vectors\n",
        "单词向量的维度\n",
        "# window: window size\n",
        "# min_count: ignores all words with total frequency lower than the specified count value\n",
        "频率比具体化数值低的单词都忽略。 （忽略总频率低语指定计数值的所有单词）\n",
        "\n",
        "# workers: Use specified number of worker threads （线程；主题）【嘶·若案·子】 to train the model (=faster training with multicore machines)\n",
        "\n",
        "运行程序就是进程 进程中最小单位是一个线程； 一个进程有至少一个线程\n",
        "清理垃圾之后再杀毒 是有顺序的，~~单线程 \n",
        "#https://blog.csdn.net/csdnnews/article/details/82321777?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161569868516780264066483%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161569868516780264066483&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-82321777.first_rank_v2_pc_rank_v29&utm_term=线程\n",
        "还讲了线程安全\n",
        "\n",
        "【worker是什么含义每天搞懂】使用指定数量的worker 线程来训练模型（=使用multicore机器进行更快的训练）\n",
        "\n",
        "# sg: training algorithm, 0 for CBOW, 1 for skip-gram\n",
        "sg：训练算法，CBOW 为0 ； Skip-gram 为1\n",
        "'''\n",
        "#这个地方空格了后面的地方没空格"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxHIwaCs4fJk"
      },
      "source": [
        "similar_words = wv_cbow_model.wv.most_similar(\"man\") #topn = 10 default (的A·佛奥·忒) 默认值\n",
        "pprint.pprint(similar_words)\n",
        "# The trained word vectors are stored in a KeyedVectors instance（实例） as model.wv\n",
        "#训练单词向量存在KeyedVectors 实例作为model.wv ([主宾谓]训练后的词向量以model.wv的形式存在keyedvec实例中)\n",
        "# Get the top 10 similar words to 'man' by calling most_similar() \n",
        "#通过calling most_similar() 得到 前十个相似单词。\n",
        "#通过调用most_similar() 获取与\"man\"相似的前十个词\n",
        "# most_similar() computes cosine similarity between a simple mean of the vectors of the given words and the vectors for each word in the model \n",
        "#他是变得。\n",
        "#most_similar() 计算余弦相似度，在一个简单的被给单词的向量含义和每个在模型中向量的单词\n",
        "#（计算给定的单词的向量和模型中每个单词的向量的简单均值之间的（ a simple mean of） 余弦相似度。\n",
        "#【不懂】\n",
        "#【值是变化的可能是获取的文件不够稳定。所有的值都是变化的。】但是频率差距不大。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CByVxtDZ4f4l"
      },
      "source": [
        "'''\n",
        "#sg=0；；cbow \n",
        "wv_cbow_model = Word2Vec(sentences = sentences, size = 100, window = 5, min_count = 5 , workers = 2, sg = 0)\n",
        "similar_words = wv_cbow_model.wv.most_similar(\"man\") \n",
        "\n",
        "#sg=1；； sg 模型\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100,window=5,min_count=5,workers=2,sg=1)\n",
        "similar_words=wv_sg_model.wv.most_similar(\"man\") \n",
        "\n",
        "ft_sg_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=1)\n",
        "result=ft_sg_model.wv.most_similar(\"electrofishing\")\n",
        "\n",
        "ft_cbow_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=0)\n",
        "result=ft_cbow_model.wv.most_similar(\"electrofishing\")\n",
        "\n",
        "\n",
        "#就是把（）中的单词变成了K-M+W （）是变化的其他是不变的\n",
        "'''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP_UQwk84i8a"
      },
      "source": [
        "'''\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "# KG 和CBOW不同的是sg这个参数多了 从0变1 这样就从CBOW 模型切换到SG模型了。\n",
        " #跟空格有关系吗？？没关系\n",
        " '''\n",
        " # Now we switch to a Skip Gram model by setting parameter sg=1\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "#不是上面是这出了问题 不是这个出了问题 与空格无关系"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lHF4dIJ4oue"
      },
      "source": [
        "similar_words=wv_sg_model.wv.most_similar(\"man\") #唯一不同把调用的模型名字变了。\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM5jChL9UCFa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPg8wzsHUCSI"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        " \n",
        "CONTEXT_SIZE = 2\n",
        "raw_text =  \"We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\".split(' ')\n",
        " \n",
        "vocab=set(raw_text)\n",
        " \n",
        "word_to_idx = {word:i for i,word in enumerate(vocab)}\n",
        " \n",
        "data=[]\n",
        " \n",
        "for i in range(CONTEXT_SIZE,len(raw_text)-CONTEXT_SIZE):\n",
        "\tcontext = [raw_text[i-2], raw_text[i-1], raw_text[i+1], raw_text[i+2]]\n",
        "\ttarget = raw_text[i]\n",
        "\tdata.append((context,target))\n",
        " \n",
        "class CBOW(nn.Module):\n",
        "\tdef __init__(self,n_word,n_dim,context_size):\n",
        "\t\tsuper(CBOW,self).__init__()\n",
        "\t\tself.embedding = nn.Embedding(n_word,n_dim)\n",
        "\t\tself.linear1= nn.Linear(2*context_size*n_dim,128)\n",
        "\t\tself.linear2 = nn.Linear(128,n_word)\n",
        "\tdef forward(self,x):\n",
        "\t\tx = self.embedding(x)\n",
        "\t\tx = x.view(1,-1)\n",
        "\t\tx = self.linear1(x)\n",
        "\t\tx = F.relu(x,inplace=True)\n",
        "\t\tx = self.linear2(x)\n",
        "\t\tx = F.log_softmax(x)\n",
        "\t\treturn x\n",
        " \n",
        "model = CBOW(len(word_to_idx),100,CONTEXT_SIZE)\n",
        "if torch.cuda.is_available():\n",
        "\tmodel = model.cuda()\n",
        " \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(),lr=1e-3)\n",
        " \n",
        "for epoch in range(1000):\n",
        "    print('epoch{}'.format(epoch))\n",
        "    print('*'*10)\n",
        "    running_loss = 0\n",
        "    for word in data:\n",
        "        context,target = word\n",
        "        context = Variable(torch.LongTensor([word_to_idx[i] for i in context]))\n",
        "        target = Variable(torch.LongTensor([word_to_idx[target]]))\n",
        "        if torch.cuda.is_available():\n",
        "        \tcontext = context.cuda()\n",
        "        \ttarget = target.cuda()\n",
        "        out = model(context)\n",
        "        loss = criterion(out,target)\n",
        "        running_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print('loss:{:.6f}'.format(running_loss/len(data)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}